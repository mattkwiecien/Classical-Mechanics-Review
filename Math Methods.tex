\documentclass{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage[parfill]{parskip}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{bm}
\DeclareMathOperator{\Lagr}{\mathcal{L}}
\DeclareMathOperator{\Ham}{\mathcal{H}}
\newcommand{\uvec}[1]{\boldsymbol{\hat{\textbf{#1}}}}
\newcommand{\vh}[1]{\hat{\vb{#1}}}
\usepackage{calligra}

\DeclareMathAlphabet{\mathcalligra}{T1}{calligra}{m}{n}
\DeclareFontShape{T1}{calligra}{m}{n}{<->s*[2.2]callig15}{}
\newcommand{\scriptr}{\mathcalligra{r}\,}
\newcommand{\boldscriptr}{\pmb{\mathcalligra{r}}\,}

\title{Math Methods}
\author{Matthew Kwiecien}
\begin{document}

\maketitle
\section{Optimization}
\subsection{Single Variable Functions}
For a function $f(x)$
\begin{align*}
	f'(a) = 0  & \implies  a \text{ is a min/max/saddle} \\
	f''(a) < 0 & \implies a \text{ is a max}             \\
	f''(a) > 0 & \implies  a \text{ is a min}            \\
	f''(a) = 0 & \implies  \text{ cannot be determined}  \\
\end{align*}
\subsection{Multi Variable Functions}
For a function $f(x,y)$
$$
\nabla f(x_0, y_0) = 0 \implies x_0, y_0 \text{ is a min/max/saddle}
$$
Now, let $H = \det \vb{H}$, where $\vb{H}$ is the Hessian Matrix
$$
\vb{H} = 
\begin{bmatrix}
	f_{xx} & f_{xy} \\
	f_{xy} & f_{yy} 
\end{bmatrix}
$$
so that
$$
H(x_0, y_0) = \det \vb{H(x_0, y_0)} = f_{xx}(x_0, y_0)f_{yy}(x_0, y_0) - f_{xy}(x_0, y_0)^2
$$
Then
\begin{align*}
	H(x_0, y_0) & = 0 \implies  (x_0, y_0) \text{ cannot be determined}      \\
	H(x_0, y_0) & < 0 \implies (x_0, y_0) \text{ is a saddle point}          \\
	H(x_0, y_0) & > 0 \implies  (x_0, y_0) \text{ is a min/max}              \\
	            & f_{xx}(x_0, y_0) > 0  \implies (x_0, y_0) \text{ is a min} \\
	            & f_{xx}(x_0, y_0) < 0  \implies (x_0, y_0) \text{ is a max} 
\end{align*}

\section{Residue Calculus}
The value of a contour integral is equal to $2\pi i$ times the sum of it's poles:
$$
\oint f(z) = 2\pi i\sum \text{Res}(f(z), z_n)
$$
where $z_n$ is a pole of $f(z)$. For a a simple pole $z_0$, The residue is given by 

$$
R(z_0) = \frac{g(z_0)}{h'(z_0)}
$$
Provided, $f(z) = g(z)/h(z)$, $g(z_0) = $ finite constant $\neq 0$, and $h(z_0) = 0$, $h'(z_0) \neq 0$.  For higher order poles, we have
$$
R(z_0) = \frac{1}{(n-1)!} \lim_{z \to z_0} \frac{d^{n-1}}{dz^{n-1}}\left( (z-z_0)^n f(z) \right)
$$

If a pole lies on the boundary of the contour, it contributes half to the value of the integral:
$$
\oint f(z) = \frac{1}{2} 2\pi i\sum \text{Res}(f(z), z_n)
$$

Some useful things to remember about Residues:
\begin{itemize}
	\item Contours follow the right hand rule (hand swings inward towards the area in the contour.)
	\item For integrals only involving $\theta$, just use the unit circle 
	      $$
	      z = e^{i\theta}, \quad dz = izd\theta, \quad \cos\theta = \frac{z + \frac{1}{z}}{2}, \quad \sin\theta = \frac{z - \frac{1}{z}}{2}
	      $$
	\item "Domain of convergence" basically where there are no poles.
	\item For functions with only imaginary roots, use a semi-circle in the complex plane, with $z = Re^{i\theta}$
	\item If there's no complex numbers in your integral, you can substitute $e^{iz}$ for sine or cosine, and take the real/imaginary part of your answer.
	\item When you choose a contour and a branch, you MUST stay within that branch.  Meaning, if you are integrating from $0$ to $2\pi$, you can't use negative angles.  
	\item When using a branch cut (like the pacman contour), generally, break it up into
	      $$
	      \oint f(z) = \int_{\Psi_1} f(z) + \int_{C_R} f(z) + \int_{\Psi_{2}} f(z) + \int_{C_r} f(z)
	      $$
	      and take the limit as $R \to \infty$, $r \to 0$, which makes the contour integrals go to 0. From there, remember that for $\Psi_1,\quad \theta = 0$, and $\Psi_{2}, \quad \theta = 2\pi$.  Then use residue theorem to get $f(z)$, set it equal to the remaining 2 integrals, and solve for your original integral.
	          
\end{itemize}


\section{Complex Analysis}
Starting from geometry in the complex plane, you get
$$
z = x + iy = r\cos\theta + i\sin\theta = re^{i\theta}
$$
Re-arranging, you get
$$
\cos\theta = \frac{e^{ix} + e^{-ix}}{2}
$$
$$
\sin\theta = \frac{e^{ix} - e^{-ix}}{2}
$$
For a complex number z,
$$
\sin z = \frac{e^{iz} - e^{-iz}}{2i}
$$
$$
\cos z = \frac{e^{iz} + e^{-iz}}{2}
$$
And if is purely imaginary, you get $z=iy$.  Evaluating cosine and sine at $z$ and taking only the imaginary part gives the hyperbolic functions:
$$
\cos z = \cos iy = \cosh z = \frac{e^z + e^{-z}}{2}
$$
$$
\sin z = \sin iy = \sinh z = \frac{e^z - e^{-z}}{2}
$$

For logs:
$$
\ln z = \ln(re^{i\theta}) = \ln(r) +  \ln(e^{i\theta}) = \ln(r) + i\theta
$$

\section{Matrices}
\subsection{Diagonalize a Matrix}
To raise the a matrix to a power, you need to first diagonalize it. To Diagonalize a matrix $\vb{A}$:

\begin{enumerate}
	\item First, find the eigenvalues by solving for $\lambda$: ($\det(\vb{A}-\lambda \vb{I})$)
	\item Use the eigenvalues to find the eigenvectors for each eigenvalue.  This usually means solve for x using
	      $$
	      (\vb{A} - \lambda \vb{I})\vb{x} = 0 \qquad \text{or} \qquad \vb{A}\vb{x} = \lambda \vb{x}
	      $$
	\item Construct the matrix $\vb{\Lambda}$ which is a diagonal matrix with the eigenvalues down the diagonal.
	      $$
	      \vb{\Lambda} = 
	      \begin{bmatrix}
	      	\lambda_1 & 0         \\
	      	0         & \lambda_2 
	      \end{bmatrix}
	      $$
	\item Construct the matrix $\vb{S}$ which each COLUMN of $\vb{S}$ is an eigenvector of $\vb{A}$.
	      $$
	      \vb{S} = 
	      \begin{bmatrix}
	      	\vb{u_1} &   
	      	\vb{u_2}
	      \end{bmatrix}
	      $$
	\item Find the inverse of $\vb{S}$, $\vb{S^{-1}}$
	\item Then you have
	      $$
	      \vb{A} = \vb{S}\vb{\Lambda}{\vb{S^{-1}}}
	      $$
\end{enumerate}

Once a matrix has been diagonalized, you can raise find the n-th power of a matrix using the diagonalized formula
$$
\vb{A^n} = \vb{S}\vb{\Lambda^n}{\vb{S^{-1}}}
$$

\subsection{Inverse of a matrix}
For a 2x2 matrix $\vb{A}$, it is simply
$$
\begin{bmatrix}
	a & b \\
	c & d 
\end{bmatrix}^{-1}
= 
\frac{1}{\det(\vb{A})}
\begin{bmatrix}
	d  & -b \\
	-c & a  
\end{bmatrix}
$$
where the determinant is simply $ad-bc$. 

For larger matrices, you need to use Gauss-Jordan method.  This uses row operations to go from $[\vb{A} | \vb{I}]$ to $[\vb{I} | \vb{A}^{-1}]$. Start with
$$
\begin{bmatrix}
	a & b & c & 1 & 0 & 0 \\
	d & e & f & 0 & 1 & 0 \\
	g & h & i & 0 & 0 & 1 \\
\end{bmatrix}
$$
and row reduce until you end up with 
$$
\begin{bmatrix}
	1 & 0 & 0 & j & k & l \\
	0 & 1 & 0 & m & n & o \\
	0 & 0 & 1 & p & q & r \\
\end{bmatrix}
$$
And the matrix on the right will be your inverse
\section{Ordinary Differential Equations}
\subsection{Systems of linear differential equations}
Given 
$$
\frac{d\vb{u}}{dt} = A \vb{u}
$$
is \textbf{linear with constant coefficients}, then these differential equations will convert directly to linear algebra, with solution 
$$
e^{\lambda t} \vb{x} \quad \text{when} \quad A\vb{x} = \lambda \vb{x}
$$

To solve this system:
\begin{enumerate}
	\item Find eigenvalues and eigenvectors as normal (see above).  Note that this requires $A$ to be diagonizable
	\item Form independent solutions from the $n$ eigenvectors + eigenvalues, e.g.
	      $$
	      \vb{u_n}(t) = e^{\lambda_n t}\vb{x_n}
	      $$
	\item Create a linear combination of the above solutions and add constants.
	      $$
	      \vb{u}(t) = C_1 e^{\lambda_1 t}\vb{x_1} + C_2 e^{\lambda_2 t}\vb{x_2} + \dots
	      $$
	\item Solve for the constants given the initial conditions.  This is usually when $t = 0$, which results in $e^{0} = 1$, and simple algebra to solve for the constants.
	\item Lastly, substitute the coefficients back into the general solution for the solution given that initial state.
	\item \textbf{NOTE} If you have a repeated eigenvalue (and therefore 1 eigenvector), use $te^{\lambda t} \vb{x}$ as your other solution.
\end{enumerate}

\section{Partial Differential Equations}
\subsection{Workflow}
If the PDE has time involved
\begin{enumerate}
	\item Separate the variables (e.g. $u(x,t)$ becomes $X(x)T(t)$)
	\item Use a constant of proportionality to create multiple ODEs 
	\item Consider the cases of your constant being $= 0$, $> 0$, $< 0$
	\item For each case, apply the boundary conditions and solve for $X(x)$ and $T(t)$
	\item Solve for the normal modes $u_n(x,t)$
	\item Assemble the general solution using two particular solutions obtained from the constant of proportionality. $u(x,t) = \sum_{n} u_n (x,t)$
	\item Incorporate the initial conditions at $t=0$, and find the Fourier Coefficients for your solution. $A_n = \dots$, $B_n = \dots$
\end{enumerate}

For a time independent, linear, homogeneous PDE.
\begin{enumerate}
	\item Separate the variables (e.g. $u(x,y)$ becomes $X(x)Y(y)$)
	\item Use a constant of proportionality to create multiple ODEs 
	\item Consider the cases of your constant being $= 0$, $> 0$, $< 0$
	\item For each case, apply the boundary conditions and solve for $X(x)$ and $Y(y)$
	\item Solve for the normal modes $u_n(\vec{r})$
	\item Assemble the general solution using two particular solutions obtained from the constant of proportionality. $u(\vec{r}) = \sum_{n} u_n (\vec{r})$
	\item Fit boundary conditions, and find the Fourier Coefficients for your solution. $A_n = \dots$, $B_n = \dots$
\end{enumerate}
\subsection{Fourier Series}
The Fourier Series is given by 
$$
f(x) = a_0 + \sum_{n=1}^{\infty} a_n \cos\frac{n\pi x}{L} + b_n \sin\frac{n\pi x}{L} 
$$

To find the Fourier Coefficients for a function, and therefore for a general solution (usually the initial condition $f(x)$), we have

$$
a_0 = \frac{1}{2L} \int_{-L}^{L} f(x) dx
\qquad
a_n = \frac{1}{L} \int_{-L}^{L} f(x) \cos\frac{n\pi x}{L} dx
\qquad
b_n = \frac{1}{L} \int_{-L}^{L} f(x) \sin\frac{n \pi x}{L} dx
$$

(Note $a_0$ is the average value).
\begin{itemize}
	\item If $f(x)$ is even, you can drop all the $b_n$ terms
	\item If $f(x)$ is odd, you can drop $a_0$, $a_n$ terms
\end{itemize}

\subsection{Fourier Transform}

This is the fourier series of an infinite domain.  Using Euler's formula, we get the transform and inverse transform:

$$
F(k) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} f(x) e^{-ikx}dx 
\qquad
f(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} F(k) e^{ikx}dk
$$

\section{Laplacian}
In cartesian coordinates
$$
\nabla \cdot \nabla = \nabla^2 = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}
$$

In cylindrical polar:

$$
\nabla \cdot \nabla = \nabla^2 = \frac{1}{\rho}\frac{\partial}{\partial \rho}\left( \rho\frac{\partial}{\partial\rho}\right) + \frac{1}{\rho^2}\frac{\partial^2}{\partial \theta^2} + \frac{\partial^2}{\partial z^2}
$$

In spherical polar:

$$
\nabla \cdot \nabla = \nabla^2 =
\frac{1}{r^2}\frac{\partial}{\partial r}\left( r^2 \frac{\partial}{\partial r} \right) + \frac{1}{r^2 \sin \theta}\frac{\partial}{\partial \theta} \left( \sin \theta \frac{\partial}{\partial \theta} \right) + \frac{1}{r^2 \sin^2 \theta} \frac{\partial^2}the{\partial \phi^2}
$$

\section{Probability}
\subsection{General probabilities}
\begin{itemize}
    \item The probability of two independent outcomes both occurring is the product of their
    probabilities
	\item The number of outcomes favorable to an event divided by the total number of outcomes.
	\item (Sample space) Probability is the sum of the probabilities associated with all the sample points favorable to the event
	\item $P(AB) = P(A) \cdot P_A(B)$ - The probability of A and B is the probability of A times the probability of B given A.
	\item $P(A+B) = P(A) + P(B) - P(AB)$ - Probability of either A or B or BOTH
	\item \textbf{Bayes' Formula} $P_A(B) = \frac{P(AB)}{P(A)}$
\end{itemize}

\subsection{Poisson Distribution}
If we are counting events that occur \textbf{independently} and \textbf{randomly} then $X$, the number of events in a fixed unit of time (or distance, area, volume), has the \textbf{Poisson Distribution}
$$
P(X=x) = \frac{\lambda^x e^{-\lambda}}{x!}
$$

The Poisson distribution tends to skew right.

\subsection{Binomial Distribution}
If there are $n$ independent trials, and each trial can either be true or false, and the probability of any trial is \textbf{constant}, then $X$ is the number of successes in the $n$ trials, given by

$$
P(X=x) = P(x) = \binom{n}{x} p^x (1-p)^{n-x}
$$
where 
$$
\binom{n}{x} = \frac{n!}{x!(n-x)!}
$$

Basically if you can reduce the distribution to either a success or a failure on any given trial, its a binomial distribution.

The formula can be thought of as the possible orderings times the probability of one specific ordering of success and failure



\subsection{Mean and Standard Deviation}
Mean or average value, 
$$
\Bar{x} = \sum_i  x_i p_i = \sum_i x_i f(x_i) 
$$
Where $p_i$ is the probability of the ith value.  For a continuous system
$$
\Bar{x} = \int_{-\infty}^{\infty} x f(x) dx
$$
is the average for a variable $x$

The variance is given by 
$$
\text{Var}(x) = \sum_{i}(x_i - \Bar{x})^2 f(x_i) = \int_{-\infty}^{\infty} (x-\Bar{x})^2 f(x) dx
$$
With standard deviation
$$
\sigma_x = \sqrt{\text{Var}(x)}
$$
The cumulative distribution function represents the probability that $x \leq x_i$ and is given by 
$$
F(x_i) = \sum_{x_j\leq x_i} f(x_{j}) = \int_{-\infty}^{x_i} f(x) dx
$$

\section{Fundamentals}
\textbf{Geometric Series:}
$$
b + bu + bu^2 + bu^3  + \dots = \sum_{k=0}^{\infty}bu^k
$$
which converges to $b/(1-u)$ if $\abs{u} < 1$, diverges if $\abs{u} \geq 1$

\textbf{Power Series}
$$
\sum_{n=0}^{\infty} c_n(x-a)^n = c_0 + c_1(x-a) + c_2(x-a)^2 + \dots
$$

\textbf{Taylor Series}:
A power series whose coefficients are derivatives of the function representing the sum
$$
\sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!} (x-a)^{k}
$$

\textbf{Ratio Test}:

If the ratio of consecutive terms of a series satisfies 
$$
\abs{\frac{_{n+1}}{a_n}} \to 1-\frac{p}{n}
$$
The series converges if $p<1$

\subsection{Calculations}
Integration by parts
$$
\int fg' dx = fg-\int f' g dx
$$

A function is \textbf{even}
$$
f(x) = f(-x)
$$
and \textbf{odd} if
$$
f(x) = -f(-x)
$$
Therefore, $\sin(x)$  is clearly odd, and $\cos(x)$ is even.
\subsection{Approximations}
Small angle approximations ($\theta \approx 0$)
$$
\sin(\theta) \approx \theta
$$

\subsection{Series Expansions}
Maclauren (e.g. Taylor at a = 0)
$$
e^{x} = 1 + \frac{x}{1!} + \frac{x^2}{2!} + \dots = \sum_{n=0}^{\infty} \frac{x^n}{n!}
$$
$$
(1+x)^\alpha = 1 + \frac{\alpha(\alpha - 1) \dots (\alpha - n + 1)}{n!}x^{n}
$$

\section{Vector Analysis}
\subsection{Separation vector}
A vector that points from a source point $\vb{r}'$ to a field point $\vb{r}$ is given by 
$$
\boldscriptr = \vb{r} - \vb{r'}, \qquad \hat{\boldscriptr} = \frac{\boldscriptr}{\scriptr} = \frac{\vb{r} - \vb{r'}}{\abs{\vb{r} - \vb{r'}}}
$$

\subsection{Differential Calculus (Div, Grad, Curl)}
The del operator is given by 
$$
\nabla = \vh{x} \frac{\partial}{\partial x} + \vh{y} \frac{\partial}{\partial y} + \vh{z} \frac{\partial}{\partial z}
$$


The \textbf{gradient} is a \textbf{vector quantity}, and defined as 
$$
\nabla T = \frac{\partial T}{\partial x} \vh{x} + \frac{\partial T}{\partial y} \vh{y} + \frac{\partial T}{\partial z} \vh{z}
$$

The gradient points in the direction of maximum increase of the function T, and the magnitude of the gradient gives the slope along this maximal direction.

The \textbf{divergence} is a \textbf{scalar quantity}, defined as

$$
\nabla \cdot \vb{v} = \frac{\partial v_x}{\partial x} + \frac{\partial v_y}{\partial y} + \frac{\partial v_z}{\partial z}
$$

The divergence is a measure of how the vector $v$ spreads out from the point in question.

The \textbf{curl} is a \textbf{vector quantity}, and defined as 
$$
\nabla \cross \vb{v}  = \left| \begin{matrix} 
    \vh{x} & \vh{y} & \vh{z} \\
    \partial / \partial x & \partial / \partial y & \partial / \partial z \\
    v_x & v_y & v_z 
  \end{matrix} \right|
$$

The curl is a measure of how much the vector $\vb{v}$ swirls around the point in question. E.g. a whirlpool would have large curl.

The \textbf{Laplacian} of a scalar is a scalar, but you can also have the laplacian of a vector.  It is defined as 

$$
\nabla^2 T = \frac{\partial^2 T}{\partial x^2} + \frac{\partial^2 T}{\partial y^2} + \frac{\partial^2 T}{\partial z^2}, \qquad \text{for a scalar }T
$$
and 
$$
\nabla^2 \vb{v} = (\nabla^2 v_x)\vh{x} + (\nabla^2 v_y) \vh{y} + (\nabla^2 v_z) \vh{z}, \qquad \text{for a vector }\vb{v}
$$

Useful properties:
\begin{itemize}
    \item The curl of a gradient is always 0 
    \item The divergence of a curl is always 0
\end{itemize}


\subsection{Integral Calculus}

A \textbf{line integral} is an expression of the form

$$
\int_{a}^{b} \vb{v} \cdot d\vb{l}, \qquad d\vb{l} = dx \vh{x} + dy \vh{y} + dz \vh{z}
$$

where $\vb{v}$ is a vector function, $d\vb{l}$ is the infinitesimal displacement vector, and the integral is carried out along a path $\mathcal{P}$ from $a \to b$. 

\begin{itemize}
    \item You want to parameterize the path so it's in terms of once variable, and have a single integral.
    \item If you have constraints, use them to make the integral in terms of 1 variable.
    \item Don't forget to use chain rule on the constraints in the displacement vector (e.g. $z=x^2 \to dz = 2x dx$
\end{itemize}

A \textbf{surface integral} is an expression of the form

$$
\int_{\mathcal{S}} \vb{v} \cdot d\vb{a}
$$

where $d\vb{a}$ is the infinitesimal patch of area perpendicular to the surface.  This usually represents the mass per unit time passing through the surface, aka the flux.  Should be 2 integrals.

A \textbf{volume integral} is an expression of the form

$$
\int_{\mathcal{V}} T d\tau, \qquad d\tau = dxdydz \quad \text{(in cartesian)}
$$

where $T$ is a scalar and $d\tau$ is the infinitesimal volume element.  This would be used to calculate the total mass given a density $T$.

For a volume integral over a \textbf{vector} function

$$
\int \vb{v} d\tau = \vh{x} \int v_x d\tau + \vh{y} \int v_y d\tau + \vh{z} v_z d\tau
$$

Hint: to parameterize a plane, if a plane has intercepts $a, b, c$ then the equation of the plane is
$$
\frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1
$$

\subsection{Fundamental Theorems}

The \textbf{fundamental theorem of calculus} says
$$
\int_a^b \left( \frac{df}{dx} \right) dx = \int_a^b F(x) dx = f(b) - f(a)
$$
So it tells you how to integrate $F(x)$: you think of a function $f(x)$ who's derivative is equal to $F$.  The basic format is that \textbf{the integral of a derivative over some region is given by the value of the functions at the end points.}

The \textbf{fundamental theorem of gradients} says 
$$
\int_a^b (\nabla T) \cdot d\vb{l} = T(\vb{b}) - T(\vb{a})
$$

Which tells you that the line integral of a gradient (or integral of a derivative) is given by the value of the function at the boundaries.
\begin{itemize}
    \item Gradients have a special property that \textbf{their line integrals are path independent.}
    \item $\int_a^b(\nabla T)\cdot d\vb{l}$ is independent of path taken from $\vb{a}$ to $\vb{b}$
    \item $\oint (\nabla T)\cdot d\vb{l} = 0$ since the beginning and end points are identical, so $T(\vb{b}) - T(\vb{a}) = 0$
\end{itemize}


The \textbf{fundamental theorem for Divergences} says
$$
\int_{\mathcal{V}} (\nabla \cdot \vb{v} )\, d\tau = \oint_{\mathcal{S}} \vb{v} \cdot d\vb{a}
$$

Also known as \textbf{Gauss's Theorem, Green's theorem, and divergence theorem}.  The integral of the divergence over a volume is equal to the value of the function on the surface that bounds the volume.

Basically
$$
\int (\text{faucets within the volume}) = \oint (\text{flow out through the surface})
$$

The \textbf{Fundamental Theorem for Curls} says

$$
\int_{\mathcal{S}}(\nabla \times \vb{v}) \cdot d\vb{a} = \oint_{\mathcal{P}} \vb{v} \cdot d\vb{l}
$$

Also known as \textbf{Stokes' Theorem}.  The integral of a curl over a patch of surface (aka the flux of the curl) is equal to the value of the function on the perimeter of the path.

\begin{itemize}
    \item The left hand side is also called the circulation
    \item $\int(\nabla \times \vb{v}) \cdot d\vb{a}$ depends only on the boundary line, not on the surface used
    \item $\int(\nabla \times \vb{v}) \cdot d\vb{a} = 0$  for any closed surface, since the boundary line shrinks like the mouth of a balloon.  So the circulation vanishes.
    \item Direction is extremely important for the loop.  Use the right hand rule, and prefer to \textbf{never use minus signs in $d\vb{l}$}.  Just reverse the bounds of integration if necessary.
\end{itemize}

\section{Integration By Parts}

You can easily derive this starting with chain rule.  Start with
\begin{align*}
(fg)' & = fg' + f'g    
\end{align*}
and integrate both sides

\begin{align*}
\int (fg)' & = \int fg' + \int f'g   \\
fg \eval_a^b & = \int fg' + \int f'g \\
\int fg'& = -\int f'g + fg \eval_a^b
\end{align*}

Now, for vector calculus, the above goes

$$
\nabla \cdot (f\vb{A}) = f(\nabla \cdot \vb{A}) + \vb{A} \cdot (\nabla f)
$$

Integrate both sides and invoke divergence theorem
$$
\int \nabla \cdot (f\vb{A}) d\tau = \int f(\nabla \cdot \vb{A}) d\tau +  \int \vb{A} \cdot (\nabla f) d\tau = \oint f\vb{A} \cdot d\vb{a}
$$
and re-arrange
$$
\int_{\mathcal{V}} f(\nabla \cdot \vb{A}) d\tau = - \int_{\mathcal{V}} \vb{A} \cdot (\nabla f) d\tau + \oint_{\mathcal{S}} f\vb{A} \cdot d\vb{a}
$$

\section{Dirac Delta Function}
The actual function is given by 
$$
\delta(x) = 
\begin{cases}
0, &\quad x \neq 0 \\
\infty, &\quad x = 0
\end{cases}
$$
and in 3 dimensions
$$
\delta^3(\vb{r}) = \delta(x) \delta(y) \delta(z)
$$
thus, it is zero everywhere except at 0 where it blows up (or (0,0,0) for 3 dimensions).

\begin{align*}
\int_{-\infty}^{\infty} \delta(x) dx & = 1 \\
\int_{\text{all space}} \delta^3{(\vb{r})} d\tau & = 1
\end{align*}

The entire point of this function is that
\begin{align*}
\int_{-\infty}^{\infty} f(x) \delta(x-a) dx & = f(a) \\ \int_{\text{all space}} f(\vb{r})\delta^3{(\vb{r-\vb{a}})} d\tau &  = f(\vb{a})
\end{align*}

Some special rules about the delta function
$$
\delta(kx) = \frac{1}{\abs{k}} \delta(x)
$$
$$
\nabla \cdot \left( \frac{\vh{r}}{r^2} \right) = 4\pi\delta^3(\vb{r})
$$

Dirac delta functions are great for "write an expression for the volume charge density" from point charges.  Just setup the delta fn to be centered at the point charges.



\section{TODO}
\begin{itemize}
	\item Poisson Distribution
	\item Gaussian Distribution
	\item Laurent Series
	\item Beta, Gamme functions
	\item Geometric Series
\end{itemize}
















\end{document}