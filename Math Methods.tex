\documentclass{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage[parfill]{parskip}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{bm}
\DeclareMathOperator{\Lagr}{\mathcal{L}}
\DeclareMathOperator{\Ham}{\mathcal{H}}
\newcommand{\uvec}[1]{\boldsymbol{\hat{\textbf{#1}}}}
\newcommand{\vh}[1]{\hat{\vb{#1}}}

\title{Math Methods}
\author{Matthew Kwiecien}
\begin{document}

\maketitle
\section{Optimization}
\subsection{Single Variable Functions}
For a function $f(x)$
\begin{align*}
    f'(a) = 0 & \implies  a \text{ is a min/max/saddle} \\
    f''(a) < 0 & \implies a \text{ is a max} \\
    f''(a) > 0 & \implies  a \text{ is a min} \\
    f''(a) = 0 & \implies  \text{ cannot be determined} \\
\end{align*}
\subsection{Multi Variable Functions}
For a function $f(x,y)$
$$
\nabla f(x_0, y_0) = 0 \implies x_0, y_0 \text{ is a min/max/saddle}
$$
Now, let $H = \det \vb{H}$, where $\vb{H}$ is the Hessian Matrix
$$
\vb{H} = 
\begin{bmatrix}
f_{xx} & f_{xy} \\
f_{xy} & f_{yy}
\end{bmatrix}
$$
so that
$$
H(x_0, y_0) = \det \vb{H(x_0, y_0)} = f_{xx}(x_0, y_0)f_{yy}(x_0, y_0) - f_{xy}(x_0, y_0)^2
$$
Then
\begin{align*}
    H(x_0, y_0) & = 0 \implies  (x_0, y_0) \text{ cannot be determined} \\
    H(x_0, y_0) & < 0 \implies (x_0, y_0) \text{ is a saddle point} \\
    H(x_0, y_0) & > 0 \implies  (x_0, y_0) \text{ is a min/max} \\
    & f_{xx}(x_0, y_0) > 0  \implies (x_0, y_0) \text{ is a min} \\
    & f_{xx}(x_0, y_0) < 0  \implies (x_0, y_0) \text{ is a max}
\end{align*}

\section{Residue Calculus}
The value of a contour integral is equal to $2\pi i$ times the sum of it's poles:
$$
\oint f(z) = 2\pi i\sum \text{Res}(f(z), z_n)
$$
where $z_n$ is a pole of $f(z)$. For a a simple pole $z_0$, The residue is given by 

$$
R(z_0) = \frac{g(z_0)}{h'(z_0)}
$$
Provided, $f(z) = g(z)/h(z)$, $g(z_0) = $ finite constant $\neq 0$, and $h(z_0) = 0$, $h'(z_0) \neq 0$.  For higher order poles, we have
$$
R(z_0) = \frac{1}{(n-1)!} \lim_{z \to z_0} \frac{d^{n-1}}{dz^{n-1}}\left( (z-z_0)^n f(z) \right)
$$

If a pole lies on the boundary of the contour, it contributes half to the value of the integral:
$$
\oint f(z) = \frac{1}{2} 2\pi i\sum \text{Res}(f(z), z_n)
$$

Some useful things to remember about Residues:
\begin{itemize}
    \item Contours follow the right hand rule (hand swings inward towards the area in the contour.)
    \item For integrals only involving $\theta$, just use the unit circle 
    $$
    z = e^{i\theta}, \quad dz = izd\theta, \quad \cos\theta = \frac{z + \frac{1}{z}}{2}, \quad \sin\theta = \frac{z - \frac{1}{z}}{2}
    $$
    \item For functions with only imaginary roots, use a semi-circle in the complex plane, with $z = Re^{i\theta}$
    \item If there's no complex numbers in your integral, you can substitute $e^{iz}$ for sine or cosine, and take the real/imaginary part of your answer.
    \item When you choose a contour and a branch, you MUST stay within that branch.  Meaning, if you are integrating from $0$ to $2\pi$, you can't use negative angles.  
    \item When using a branch cut (like the pacman contour), generally, break it up into
    $$
    \oint f(z) = \int_{\Psi_1} f(z) + \int_{C_R} f(z) + \int_{Psi_{2}} f(z) + \int_{C_r} f(z)
    $$
    and take the limit as $R \to \infty$, $r \to 0$, which makes the contour integrals go to 0. From there, remember that for $\Psi_1,\quad \theta = 0$, and $\Psi_{2}, \quad \theta = 2\pi$.  Then use residue theorem to get $f(z)$, set it equal to the remaining 2 integrals, and solve for your original integral.
    
\end{itemize}


\section{Complex Analysis}
Starting from geometry in the complex plane, you get
$$
z = x + iy = r\cos\theta + i\sin\theta = re^{i\theta}
$$
Re-arranging, you get
$$
\cos\theta = \frac{e^{ix} + e^{-ix}}{2}
$$
$$
\sin\theta = \frac{e^{ix} - e^{-ix}}{2}
$$
For a complex number z,
$$
\sin z = \frac{e^{iz} - e^{-iz}}{2i}
$$
$$
\cos z = \frac{e^{iz} + e^{-iz}}{2}
$$
And if is purely imaginary, you get $z=iy$.  Evaluating cosine and sine at $z$ and taking only the imaginary part gives the hyperbolic functions:
$$
\cos z = \cos iy = \cosh z = \frac{e^z + e^{-z}}{2}
$$
$$
\sin z = \sin iy = \sinh z = \frac{e^z - e^{-z}}{2}
$$

For logs:
$$
\ln z = \ln(re^{i\theta}) = \ln(r) +  \ln(e^{i\theta}) = \ln(r) + i\theta
$$

\section{Matrices}
\subsection{Diagonalize a Matrix}
To raise the a matrix to a power, you need to first diagonalize it. To Diagonalize a matrix $\vb{A}$:

\begin{enumerate}
    \item First, find the eigenvalues by solving for $\lambda$: ($\det(\vb{A}-\lambda \vb{I})$)
    \item Use the eigenvalues to find the eigenvectors for each eigenvalue.  This usually means solve for x using
    $$
    (\vb{A} - \lambda \vb{I})\vb{x} = 0 \qquad \text{or} \qquad \vb{A}\vb{x} = \lambda \vb{x}
    $$
    \item Construct the matrix $\vb{\Lambda}$ which is a diagonal matrix with the eigenvalues down the diagonal.
    $$
    \vb{\Lambda} = 
        \begin{bmatrix}
        \lambda_1 & 0 \\
        0 & \lambda_2
        \end{bmatrix}
    $$
    \item Construct the matrix $\vb{S}$ which each COLUMN of $\vb{S}$ is an eigenvector of $\vb{A}$.
    $$
    \vb{S} = 
        \begin{bmatrix}
        \vb{u_1} &
        \vb{u_2}
        \end{bmatrix}
    $$
    \item Find the inverse of $\vb{S}$, $\vb{S^{-1}}$
    \item Then you have
    $$
    \vb{A} = \vb{S}\vb{\Lambda}{\vb{S^{-1}}}
    $$
\end{enumerate}

Once a matrix has been diagonalized, you can raise find the n-th power of a matrix using the diagonalized formula
$$
\vb{A^n} = \vb{S}\vb{\Lambda^n}{\vb{S^{-1}}}
$$

\subsection{Inverse of a matrix}
For a 2x2 matrix $\vb{A}$, it is simply
$$
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}^{-1}
 = 
 \frac{1}{\det(\vb{A})}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
$$
where the determinant is simply $ad-bc$. 

For larger matrices, you need to use Gauss-Jordan method.  This uses row operations to go from $[\vb{A} | \vb{I}]$ to $[\vb{I} | \vb{A}^{-1}]$. Start with
$$
\begin{bmatrix}
a & b & c & 1 & 0 & 0 \\
d & e & f & 0 & 1 & 0 \\
g & h & i & 0 & 0 & 1 \\
\end{bmatrix}
$$
and row reduce until you end up with 
$$
\begin{bmatrix}
1 & 0 & 0 & j & k & l \\
0 & 1 & 0 & m & n & o \\
0 & 0 & 1 & p & q & r \\
\end{bmatrix}
$$
And the matrix on the right will be your inverse
\end{document}
